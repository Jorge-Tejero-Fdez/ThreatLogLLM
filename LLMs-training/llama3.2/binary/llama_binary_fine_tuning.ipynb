{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK LLAMA 3.2 - BINARY CLASSIFICATION (FINE-TUNING)\n",
    "\n",
    "This notebook evaluates the LLama LLM using fine-tuning prompting for binary classification, distinguishing between normal and attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objectives\n",
    "\n",
    "The objective of this notebook is to evaluate the performance of the Llama LLM in a **fine-tuned binary classification** setting.\n",
    "\n",
    "Specifically, this notebook aims to:\n",
    "- Assess the model's ability to distinguish between **normal** and **attack** after being fine-tuned on labeled examples.\n",
    "- Analyze its predictions and evaluate its suitability for binary threat detection tasks in IoT systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. IMPORTS AND SETUP\n",
    "Import the required python libraries, preprocessing, training (fine-tuning) and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/TFM/threatlogllm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas, json, re and os imports\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "# visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# imports for LLM training\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import interpreter_login\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "# imports for evaluation (sklearn)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# tqdm import\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Login to Hugging Face Hub\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PREPARE DATASET\n",
    "\n",
    "The dataset is split into 70% for training, 15% for validation, and 15% for testing. The model is fine-tuned on the training set while using the validation set to monitor performance during training. After fine-tuning, the final evaluation is performed on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n### Instruction:\\nYou are a cybersecurity expert specializing in IoT security. Your task is to analyze network logs and determine whether the given log data indicates a potential attack. Only provide a response if this log data indicates a potential attack or normal traffic.\\n### Question:\\n- The length of the DNS query is: 0\\n- The MQTT protocol name used is: 0\\n- The MQTT message type is: 0\\n- The MQTT topic is: 0\\n- The MQTT connection acknowledgment flags are: 0\\n- TCP options set in the packet are: 0\\n- TCP destination port is: 53316.0\\n### Response:\\n<think>\\nThis log data is normal traffic.\\n</think>'}\n"
     ]
    }
   ],
   "source": [
    "# Define file path\n",
    "file_path = \"../../../data/prompts/binary_instructions.jsonl\"\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "# Read JSONL file line by line\n",
    "samples = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        sample = json.loads(line.strip())  # Load each line as a JSON object\n",
    "        samples.append({\"text\": sample[\"Prompt\"]})  # Extract the prompt field\n",
    "\n",
    "# Convert into Hugging Face Dataset format\n",
    "dataset_dict = {\"full\": Dataset.from_list(samples)}\n",
    "dataset = DatasetDict(dataset_dict)\n",
    "\n",
    "# Verify first example\n",
    "print(dataset[\"full\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples: 8400\n",
      "Validation Samples: 1800\n",
      "Test Samples: 1800\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the dataset\n",
    "dataset = dataset[\"full\"].shuffle(seed=42)\n",
    "\n",
    "# Split dataset into train (70%), validation (15%), test (15%)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "\n",
    "# Ensure the split sizes are correct\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))\n",
    "test_dataset = dataset.select(range(train_size + val_size, len(dataset)))\n",
    "\n",
    "# Print sizes\n",
    "print(f\"Train Samples: {len(train_dataset)}\")\n",
    "print(f\"Validation Samples: {len(val_dataset)}\")\n",
    "print(f\"Test Samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PREPARE LLM\n",
    "\n",
    "Load the Llama language model from Unsloth with specific configuration parameters for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4060 Laptop GPU. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Define model loading parameters\n",
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True\n",
    "\n",
    "# Load the Llama LLM with Unsloth's FastLanguageModel utility\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (1): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (2-27): 26 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the model's config\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  \n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\",\n",
    "        \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  \n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  \n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# Reach the model for training\n",
    "FastLanguageModel.for_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PREPARE DATASET WITH EOS\n",
    "\n",
    "In this section, each sample in the dataset is formatted to include an end-of-sentence (EOS) token.  \n",
    "This ensures proper termination of sequences during training and improves model understanding of prompt boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8400/8400 [00:00<00:00, 93667.91 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1800/1800 [00:00<00:00, 103078.07 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1800/1800 [00:00<00:00, 99921.21 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
      "### Instruction:\n",
      "You are a cybersecurity expert specializing in IoT security. Your task is to analyze network logs and determine whether the given log data indicates a potential attack. Only provide a response if this log data indicates a potential attack or normal traffic.\n",
      "### Question:\n",
      "- The length of the DNS query is: 0.0\n",
      "- The MQTT protocol name used is: 0.0\n",
      "- The MQTT message type is: 0.0\n",
      "- The MQTT topic is: 0.0\n",
      "- The MQTT connection acknowledgment flags are: 0.0\n",
      "- TCP options set in the packet are: 0\n",
      "- TCP destination port is: 0.0\n",
      "### Response:\n",
      "<think>\n",
      "This log data is an attack!!.\n",
      "</think><|end_of_text|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Ensure each training sample has an EOS token\n",
    "\n",
    "# Formatting function to structure data properly\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for entry in examples[\"text\"]:  # \"text\" is the key in our dataset\n",
    "        text = entry + EOS_TOKEN  # Append EOS token\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "val_dataset = val_dataset.map(formatting_prompts_func, batched=True)\n",
    "test_dataset = test_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Print an example formatted sample\n",
    "print(train_dataset[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. INITIALIZE THE SFTTrainer FOR FINE-TUNING\n",
    "\n",
    "This section initializes the `SFTTrainer` with the model, tokenizer, and prepared datasets.  \n",
    "It sets training arguments such as learning rate, batch size, evaluation strategy, and checkpointing.  \n",
    "The model will be fine-tuned using supervised learning and validated on the evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2):   0%|          | 0/8400 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8400/8400 [00:03<00:00, 2765.44 examples/s]\n",
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1800/1800 [00:03<00:00, 474.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the trainer, with SFTTrainer and TrainingArguments\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=500,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs_llama\",\n",
    "        eval_strategy=\"steps\",  \n",
    "        save_strategy=\"steps\", \n",
    "        save_total_limit=2,  \n",
    "        metric_for_best_model=\"eval_loss\", \n",
    "        load_best_model_at_end=True,  \n",
    "        greater_is_better=False,  \n",
    "    ),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Stop if loss doesn't improve in 3 evaluations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. TRAINING MODEL\n",
    "\n",
    "In this section, the model is fine-tuned for up to 500 steps.  \n",
    "Evaluation is performed on the validation set every 10 steps, and early stopping is applied if no improvement is observed for 3 consecutive evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 8,400 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 500\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 2:16:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.192700</td>\n",
       "      <td>0.941043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.178336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.140197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>0.100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.089100</td>\n",
       "      <td>0.095130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>0.089407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.084437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.086235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.080596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.081815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.080051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.083680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.078602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.078618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>0.077040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.077298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.076876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.077283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.078529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.075147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.074954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.074756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.068900</td>\n",
       "      <td>0.074128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.068400</td>\n",
       "      <td>0.073334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.073113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.073181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.072410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.071989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.072433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.091300</td>\n",
       "      <td>0.071896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.088500</td>\n",
       "      <td>0.071688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.071772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.071119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.070672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.070101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.070185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.069592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.069346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.078400</td>\n",
       "      <td>0.069652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.068884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>0.068724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.068293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.068177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.068368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>0.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.068264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.067757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>0.067699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "# Start fine-tuning\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine-tuned-model/llama_binary_finetuned_model/tokenizer_config.json',\n",
       " 'fine-tuned-model/llama_binary_finetuned_model/special_tokens_map.json',\n",
       " 'fine-tuned-model/llama_binary_finetuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save fine-tuned model and tokenizer\n",
    "trainer.save_model(\"fine-tuned-model/llama_binary_finetuned_model\")  # Saves model weights\n",
    "tokenizer.save_pretrained(\"fine-tuned-model/llama_binary_finetuned_model\")  # Saves tokenizer config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. COMPUTE PREDICTIONS\n",
    "\n",
    "In this section, predictions are generated using the LLM (`predictions_binary`) and stored in a list. The corresponding true labels are also collected in a separate list (`actual_labels_binary`). \n",
    "\n",
    "Both lists will be used later to compute evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure labels are properly converted to binary\n",
    "def label_to_binary(response):\n",
    "    response = response.lower()\n",
    "    if \"attack\" in response:\n",
    "        return 1  # Attack detected\n",
    "    return 0  # Normal traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Samples:   0%|          | 0/1800 [00:00<?, ?sample/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1800/1800 [34:00<00:00,  1.13s/sample]\n"
     ]
    }
   ],
   "source": [
    "# Ensures optimized inference\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "# Initialize lists to store actual vs predicted labels\n",
    "predictions_binary = []\n",
    "actual_labels_binary = []\n",
    "\n",
    "# Iterate over test dataset with tqdm for progress tracking\n",
    "for sample in tqdm(test_dataset, desc=\"Processing Samples\", unit=\"sample\"):\n",
    "    full_text = sample[\"text\"]\n",
    "\n",
    "    # Extract only the log entry (remove the response part)\n",
    "    input_text = re.split(r\"### Response:\", full_text, maxsplit=1)[0].strip()\n",
    "\n",
    "    # Extract actual label correctly (but don't pass it to the model)\n",
    "    actual_label = label_to_binary(full_text.split(\"### Response:\")[1].strip()) if \"### Response:\" in full_text else label_to_binary(\"Normal\")\n",
    "\n",
    "    # Tokenize input (ONLY the log entry)\n",
    "    inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=200,  # Keep short to avoid unnecessary text\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    # Decode prediction and convert to binary\n",
    "    predicted_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "    predicted_label = label_to_binary(predicted_text.split(\"### Response:\")[1].strip())\n",
    "\n",
    "    # Store results\n",
    "    predictions_binary.append(predicted_label)\n",
    "    actual_labels_binary.append(actual_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compute Evaluation Metrics\n",
    "\n",
    "In this section, standard evaluation metrics are computed to assess the model's performance. These include Accuracy, Precision, Recall, F1-Score, and the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation\n",
    "accuracy = accuracy_score(actual_labels_binary, predictions_binary)\n",
    "precision = precision_score(actual_labels_binary, predictions_binary, average=\"binary\", pos_label=1)\n",
    "recall = recall_score(actual_labels_binary, predictions_binary, average=\"binary\", pos_label=1)\n",
    "f1 = f1_score(actual_labels_binary, predictions_binary, average=\"binary\", pos_label=1)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAHWCAYAAADuNVprAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXv9JREFUeJzt3Xlcjen/P/DXOalT2kvroJBJKTSZIYYskXUYxu5TYjCGsWSfsW8NY2eMZSyNsY2xjLFOY8uSGGQbGktkKQqVRNG5fn/4db6OiuKcbnW/nh7n8XCu+zrX/b5PZ3mfa7lvhRBCgIiIiGRHKXUAREREJA0mAURERDLFJICIiEimmAQQERHJFJMAIiIimWISQEREJFNMAoiIiGSKSQAREZFMMQkgIiKSKSYBErp8+TKaNm0KS0tLKBQKbN26VaftX79+HQqFAqtWrdJpu8VZgwYN0KBBA6nDyFePHj3g6uoqdRh65+rqih49ekgdxltbvXo1qlSpAkNDQ1hZWQF4/19b75Pi/vcvSWSfBFy9ehV9+/ZFxYoVYWxsDAsLC9StWxfz5s3DkydP9Lrv4OBgnDt3DlOnTsXq1atRs2ZNve6vKPXo0QMKhQIWFhZ5Po+XL1+GQqGAQqHAzJkzC93+nTt3MGHCBMTExOgg2qLh6uqKVq1aSR2GXhw4cEDz98y52djYoHbt2lizZo3U4enUpUuX0KNHD1SqVAnLli3D0qVLpQ4JO3fuxIQJE6QOg4qhUlIHIKUdO3agQ4cOUKlUCAoKgpeXF7KysnD48GEMHz4cFy5c0Nsb/MmTJ4iKisJ3332HAQMG6GUfLi4uePLkCQwNDfXS/puUKlUKGRkZ+PPPP9GxY0etbWvWrIGxsTGePn36Vm3fuXMHEydOhKurK2rUqFHgx/31119vtT8qmIEDB+Ljjz8GANy/fx8bNmxA9+7dkZKSgv79+2vqxcbGQqksnr9BDhw4ALVajXnz5sHNzU1TLuVra+fOnfjxxx+ZCFChyTYJiIuLQ+fOneHi4oJ9+/bByclJs61///64cuUKduzYobf9JyUlAYCmK1EfFAoFjI2N9db+m6hUKtStWxfr1q3LlQSsXbsWLVu2xKZNm4okloyMDJQuXRpGRkZFsj+5qlevHr744gvN/X79+qFixYpYu3atVhKgUqmkCE/zOngX9+7dA5D7vcvXFhVHxTMV14EZM2YgPT0dy5cv10oAcri5uWHQoEGa+8+fP8fkyZNRqVIlqFQquLq64ttvv0VmZqbW43K6fA8fPoxPPvkExsbGqFixIn755RdNnQkTJsDFxQUAMHz4cCgUCs04cH5jwhMmTIBCodAqi4iIwKeffgorKyuYmZnB3d0d3377rWZ7fnMC9u3bh3r16sHU1BRWVlZo06YNLl68mOf+rly5gh49esDKygqWlpYICQlBRkZG/k/sK7p27Ypdu3YhJSVFU3bixAlcvnwZXbt2zVX/wYMHGDZsGLy9vWFmZgYLCws0b94cZ86c0dQ5cOCA5tdmSEiIpvs55zgbNGgALy8vnDx5EvXr10fp0qU1z8ur47bBwcEwNjbOdfyBgYGwtrbGnTt3Cnys+jJz5kzUqVMHtra2MDExga+vL37//fdc9RQKBQYMGICNGzfC09MTJiYm8PPzw7lz5wAAS5YsgZubG4yNjdGgQQNcv35d6/GHDh1Chw4dUL58eahUKpQrVw5Dhgx5p2ExIyMjWFtbo1Qp7d8br44Jr1q1CgqFAkeOHEFoaCjs7OxgamqKzz//XJMw5/jjjz/QsmVLODs7Q6VSoVKlSpg8eTKys7O16uX3OggODkaZMmXw7NmzXPE2bdoU7u7u+R6Pq6srxo8fDwCws7ODQqHQ/Pp+9bWVM0Ty22+/YerUqShbtiyMjY3RuHFjXLlyJVfb0dHRaNasGSwtLVG6dGn4+/vjyJEj+caSo0ePHvjxxx8BQGs45uUYDhw4oPWYvD4bevToATMzM9y+fRtt27aFmZkZ7OzsMGzYsFzPrVqtxty5c1G1alUYGxvDwcEBffv2xcOHD7XqCSEwZcoUlC1bFqVLl0bDhg1x4cKFNx4TFR3Z9gT8+eefqFixIurUqVOg+l9++SXCw8PxxRdfYOjQoYiOjkZYWBguXryILVu2aNW9cuUKvvjiC/Tq1QvBwcFYsWIFevToAV9fX1StWhXt2rWDlZUVhgwZgi5duqBFixYwMzMrVPwXLlxAq1atUK1aNUyaNAkqlQpXrlx544fG33//jebNm6NixYqYMGECnjx5ggULFqBu3bo4depUrgSkY8eOqFChAsLCwnDq1Cn8/PPPsLe3x/Tp0wsUZ7t27fDVV19h8+bN6NmzJ4AXvQBVqlTBRx99lKv+tWvXsHXrVnTo0AEVKlTA3bt3sWTJEvj7++Pff/+Fs7MzPDw8MGnSJIwbNw59+vRBvXr1AEDrb3n//n00b94cnTt3Rvfu3eHg4JBnfPPmzcO+ffsQHByMqKgoGBgYYMmSJfjrr7+wevVqODs7F+g49WnevHn47LPP0K1bN2RlZWH9+vXo0KEDtm/fjpYtW2rVPXToELZt26b51R0WFoZWrVphxIgRWLRoEb7++ms8fPgQM2bMQM+ePbFv3z7NYzdu3IiMjAz069cPtra2OH78OBYsWIBbt25h48aNBYr10aNHSE5OBvAioVu7di3Onz+P5cuXF+jx33zzDaytrTF+/Hhcv34dc+fOxYABA7BhwwZNnVWrVsHMzAyhoaEwMzPDvn37MG7cOKSlpeGHH37Qai+v14GpqSl++eUX7NmzR2uORmJiIvbt26f5ks/L3Llz8csvv2DLli346aefYGZmhmrVqr32mL7//nsolUoMGzYMqampmDFjBrp164bo6GhNnX379qF58+bw9fXF+PHjoVQqsXLlSjRq1AiHDh3CJ598km/7ffv2xZ07dxAREYHVq1e/NpY3yc7ORmBgIGrVqoWZM2fi77//xqxZs1CpUiX069dPa5+rVq1CSEgIBg4ciLi4OCxcuBCnT5/GkSNHNEOQ48aNw5QpU9CiRQu0aNECp06dQtOmTZGVlfVOcZIOCRlKTU0VAESbNm0KVD8mJkYAEF9++aVW+bBhwwQAsW/fPk2Zi4uLACAiIyM1Zffu3RMqlUoMHTpUUxYXFycAiB9++EGrzeDgYOHi4pIrhvHjx4uX/1xz5swRAERSUlK+cefsY+XKlZqyGjVqCHt7e3H//n1N2ZkzZ4RSqRRBQUG59tezZ0+tNj///HNha2ub7z5fPg5TU1MhhBBffPGFaNy4sRBCiOzsbOHo6CgmTpyY53Pw9OlTkZ2dnes4VCqVmDRpkqbsxIkTuY4th7+/vwAgFi9enOc2f39/rbI9e/YIAGLKlCni2rVrwszMTLRt2/aNx1hYLi4uomXLlq+tk9ffPyMjQ+t+VlaW8PLyEo0aNdIqByBUKpWIi4vTlC1ZskQAEI6OjiItLU1TPnr0aAFAq+6r+xFCiLCwMKFQKMSNGzdeG/f+/fsFgFw3pVIppk6dmqu+i4uLCA4O1txfuXKlACACAgKEWq3WlA8ZMkQYGBiIlJSU18bZt29fUbp0afH06VNNWX6vg+zsbFG2bFnRqVMnrfLZs2cLhUIhrl279tpjzXlvvPree/W1lfOceHh4iMzMTE35vHnzBABx7tw5IYQQarVaVK5cWQQGBmode0ZGhqhQoYJo0qTJa+MRQoj+/fuLvD7Oc2LYv3+/Vnlenw3BwcECgNb7TAghfHx8hK+vr+b+oUOHBACxZs0arXq7d+/WKr93754wMjISLVu21Dqub7/9VgDQ+vuTdGQ5HJCWlgYAMDc3L1D9nTt3AgBCQ0O1yocOHQoAueYOeHp6an6dAi+6Dd3d3XHt2rW3jvlVOeORf/zxB9RqdYEek5CQgJiYGPTo0QM2Njaa8mrVqqFJkyaa43zZV199pXW/Xr16uH//vuY5LIiuXbviwIEDml9aiYmJeQ4FAC/GinMmjGVnZ+P+/fuaoY5Tp04VeJ8qlQohISEFqtu0aVP07dsXkyZNQrt27WBsbIwlS5YUeF/6ZmJiovn/w4cPkZqainr16uX5fDRu3FirN6dWrVoAgPbt22u93nPKX35Nvryfx48fIzk5GXXq1IEQAqdPny5QrOPGjUNERAQiIiKwYcMGdOnSBd999x3mzZtXoMf36dNHa9irXr16yM7Oxo0bN/KMM6fnoV69esjIyMClS5e02svrdaBUKtGtWzds27YNjx490pSvWbMGderUQYUKFQoUa0GFhIRozRfI+WzIee5jYmI0w2P3799HcnIykpOT8fjxYzRu3BiRkZEFfo/rQl7v+ZdfJxs3boSlpSWaNGmiiTU5ORm+vr4wMzPD/v37AbzodczKysI333yj9TcdPHhwkRwHFYwshwMsLCwAQOsD4HVu3LgBpVKpNRMYABwdHWFlZaX1AQUA5cuXz9WGtbV1rvGyd9GpUyf8/PPP+PLLLzFq1Cg0btwY7dq1wxdffJHvrOucOPMa8/Tw8MCePXvw+PFjmJqaaspfPRZra2sAL76Mcp7HN2nRogXMzc2xYcMGxMTE4OOPP4abm1uuMWkAmlnXixYtQlxcnNZYpK2tbYH2BwAffPBBoSZqzZw5E3/88QdiYmKwdu1a2Nvbv/ExSUlJWvGZmZkVelinILZv344pU6YgJiZGaw7Kq3NEgNx/L0tLSwBAuXLl8ix/+TUZHx+PcePGYdu2bbleq6mpqQWK1dvbGwEBAZr7HTt2RGpqKkaNGoWuXbvCzs7utY9/3estx4ULFzBmzBjs27cvVzL6apz5vQ6CgoIwffp0bNmyBUFBQYiNjcXJkyexePHiAh1nYbzpmC5fvgzgxfyU/KSmpsLU1BQPHjzQKrezs4OBgYHOYjU2Ns71N3r1s+vy5ctITU3N9z2SM3Ey5/OmcuXKuWLOeQ5IerJNApydnXH+/PlCPS6vD9285PemFEK89T5enZhjYmKCyMhI7N+/Hzt27MDu3buxYcMGNGrUCH/99ZfOPhje5VhyqFQqtGvXDuHh4bh27dprlzFNmzYNY8eORc+ePTF58mTY2NhAqVRi8ODBhfo19PKvxYI4ffq05sPr3Llz6NKlyxsf8/HHH2slgOPHj9f5Eq1Dhw7hs88+Q/369bFo0SI4OTnB0NAQK1euxNq1a3PVz+/v9aa/Y3Z2Npo0aYIHDx5g5MiRqFKlCkxNTXH79m306NHjnX6JNm7cGNu3b8fx48dzzWEobJwpKSnw9/eHhYUFJk2ahEqVKsHY2BinTp3CyJEjc8WZ3+vA09MTvr6++PXXXxEUFIRff/0VRkZGuVax6MKbjikn5h9++CHf5a5mZmY4cuQIGjZsqFUeFxf32pNLFfTz5E2xvkytVsPe3j7f8z+8KdGj94sskwAAaNWqFZYuXYqoqCj4+fm9tq6LiwvUajUuX74MDw8PTfndu3eRkpKimemvC9bW1loz6XO82tsAvOjWbNy4MRo3bozZs2dj2rRp+O6777B//36tX2MvHwfwYo32qy5duoQyZcpo9QLoUteuXbFixQoolUp07tw533q///47GjZsmGsiWUpKCsqUKaO5X9CErCAeP36MkJAQeHp6ok6dOpgxYwY+//xzzQqE/KxZs0Zr5nzFihV1FlOOTZs2wdjYGHv27NFaVrdy5Uqd7ufcuXP477//EB4ejqCgIE15RETEO7f9/PlzAEB6evo7t3XgwAHcv38fmzdvRv369TXlcXFxhW4rKCgIoaGhSEhI0CxZleIXaqVKlQC8+HGS1/s2R/Xq1XP9PRwdHQHk/37IOZ5XP1Py+jwpqEqVKuHvv/9G3bp1X5ts53zeXL58Weu9kZSUpNNeUXo3spwTAAAjRoyAqakpvvzyS9y9ezfX9qtXr2rGMVu0aAHgxczgl82ePRsA3vjrpjAqVaqE1NRUnD17VlOWkJCQawXCq92CADS/Il5dtpjDyckJNWrUQHh4uNaHwvnz5/HXX39pjlMfGjZsiMmTJ2PhwoWaD668GBgY5Opl2LhxI27fvq1VlpOs5JUwFdbIkSMRHx+P8PBwzJ49G66urggODs73ecxRt25dBAQEaG76SAIMDAygUCi0frldv35d56eYzvkF+PJzL4Qo8Fj+62zfvh3Aiy+xd5VXnFlZWVi0aFGh2+rSpQsUCgUGDRqEa9euoXv37u8c39vw9fVFpUqVMHPmzDwTpZwlktbW1lqvt4CAAM15QPJ7P7i4uMDAwACRkZFa5W/zfOXo2LEjsrOzMXny5Fzbnj9/rokhICAAhoaGWLBggdbf69XPUZKWbHsCKlWqhLVr16JTp07w8PDQOmPg0aNHsXHjRs065urVqyM4OBhLly7VdEceP34c4eHhaNu2ba4uunfRuXNnjBw5Ep9//jkGDhyIjIwM/PTTT/jwww+1JoJNmjQJkZGRaNmyJVxcXHDv3j0sWrQIZcuWxaeffppv+z/88AOaN28OPz8/9OrVS7NE0NLSUq9nG1MqlRgzZswb67Vq1QqTJk1CSEgI6tSpg3PnzmHNmjW5vmArVaoEKysrLF68GObm5jA1NUWtWrUKPalr3759WLRoEcaPH69Zsrhy5Uo0aNAAY8eOxYwZMwrV3ptcuXIFU6ZMyVXu4+OTZzLZsmVLzJ49G82aNUPXrl1x7949/Pjjj3Bzc9NKFN9VlSpVUKlSJQwbNgy3b9+GhYUFNm3aVOhfbIcOHdKcBfLBgwfYtm0bDh48iM6dO6NKlSrvHGedOnVgbW2N4OBgDBw4EAqFAqtXry7U8FQOOzs7NGvWDBs3boSVlZVOk/nCUCqV+Pnnn9G8eXNUrVoVISEh+OCDD3D79m3s378fFhYW+PPPP1/bhq+vL4AXZ2wMDAyEgYEBOnfuDEtLS3To0AELFiyAQqFApUqVsH37ds3Q19vw9/dH3759ERYWhpiYGDRt2hSGhoa4fPkyNm7ciHnz5uGLL77QnGMgZ5lqixYtcPr0aezatUurV48kJs2ihPfHf//9J3r37i1cXV2FkZGRMDc3F3Xr1hULFizQWm707NkzMXHiRFGhQgVhaGgoypUrJ0aPHq1VR4j8l4G9unwovyWCQgjx119/CS8vL2FkZCTc3d3Fr7/+mmuJ4N69e0WbNm2Es7OzMDIyEs7OzqJLly7iv//+y7WPV5fR/f3336Ju3brCxMREWFhYiNatW4t///1Xq05+y6BylnK9vLQsLy8vEcxPfksEhw4dKpycnISJiYmoW7euiIqKynNp3x9//CE8PT1FqVKltI7T399fVK1aNc99vtxOWlqacHFxER999JF49uyZVr0hQ4YIpVIpoqKiXnsMhZGzfDSvW69evYQQeS8RXL58uahcubJQqVSiSpUqYuXKlbleD0K8WCLYv39/rbL8Xmc5S8c2btyoKfv3339FQECAMDMzE2XKlBG9e/cWZ86cyXcpZl7tvXwzMjISVapUEVOnThVZWVm5nou8lgieOHEiz3ZfXuJ25MgRUbt2bWFiYiKcnZ3FiBEjNMs8X673utdBjt9++00AEH369HltvZcVdongy8+xEPm/L0+fPi3atWsnbG1thUqlEi4uLqJjx45i7969b4zp+fPn4ptvvhF2dnZCoVBovTaSkpJE+/btRenSpYW1tbXo27evOH/+fJ5LBPN6z+b1WhNCiKVLlwpfX19hYmIizM3Nhbe3txgxYoS4c+eOpk52draYOHGi5v3coEEDcf78+Vx/f5KOQoi3SKGJiEqAP/74A23btkVkZKTWsl4iuWASQESy1apVK1y8eBFXrlzR6WRTouJCtnMCiEi+1q9fj7Nnz2LHjh2YN28eEwCSLfYEEJHsKBQKmJmZoVOnTli8eHGuCxwRyQVf+UQkO/ztQ/SCbM8TQEREJHdMAoiIiGSKSQAREZFMcU4AERHJlonPAJ219eT0Qp21VVRKbBJg0miq1CEQ6dWTfd/h6XOpoyDSL2N9f0sp5N0hLu+jJyIikrES2xNARET0RjI/URSTACIiki8OBxAREZEcsSeAiIjki8MBREREMsXhACIiIpIj9gQQEZF8cTiAiIhIpjgcQERERHLEngAiIpIvDgcQERHJFIcDiIiISI7YE0BERPLF4QAiIiKZ4nAAERERyRF7AoiISL44HEBERCRTHA4gIiIiOWJPABERyZfMewKYBBARkXwp5T0nQN4pEBERkYyxJ4CIiOSLwwFEREQyJfMlgvJOgYiIiGSMPQFERCRfHA4gIiKSKQ4HEBERkRyxJ4CIiOSLwwFEREQyxeEAIiIikiP2BBARkXxxOICIiEimOBxAREREcsSeACIiki8OBxAREckUhwOIiIhIjtgTQERE8sXhACIiIpmSeRIg76MnIiKSMfYEEBGRfMl8YiCTACIiki8OBxAREZEcsSeAiIjki8MBREREMsXhACIiIpIj9gQQEZF8cTiAiIhInhQyTwI4HEBERFTEsrOzMXbsWFSoUAEmJiaoVKkSJk+eDCGEpo4QAuPGjYOTkxNMTEwQEBCAy5cva7Xz4MEDdOvWDRYWFrCyskKvXr2Qnp5e4DgkTwLS0tLy3XblypUijISIiORGoVDo7FYY06dPx08//YSFCxfi4sWLmD59OmbMmIEFCxZo6syYMQPz58/H4sWLER0dDVNTUwQGBuLp06eaOt26dcOFCxcQERGB7du3IzIyEn369ClwHJInAS1btkRmZmau8tjYWDRo0KDoAyIiIvlQ6PBWCEePHkWbNm3QsmVLuLq64osvvkDTpk1x/PhxAC96AebOnYsxY8agTZs2qFatGn755RfcuXMHW7duBQBcvHgRu3fvxs8//4xatWrh008/xYIFC7B+/XrcuXOnQHFIngSYmZnh888/x/PnzzVlFy9eRIMGDdC+fXsJIyMiIiq4zMxMpKWlad3y+pELAHXq1MHevXvx33//AQDOnDmDw4cPo3nz5gCAuLg4JCYmIiAgQPMYS0tL1KpVC1FRUQCAqKgoWFlZoWbNmpo6AQEBUCqViI6OLlDMkicBmzdvRmpqKrp16wYhBM6fP48GDRqgS5cumDdvntThERFRCabL4YCwsDBYWlpq3cLCwvLc76hRo9C5c2dUqVIFhoaG8PHxweDBg9GtWzcAQGJiIgDAwcFB63EODg6abYmJibC3t9faXqpUKdjY2GjqvInkqwNMTEywY8cONGjQAB07dkRkZCSCgoLwww8/SB0aERGVcLpcHTB69GiEhoZqlalUqjzr/vbbb1izZg3Wrl2LqlWrIiYmBoMHD4azszOCg4N1FtObSJIEvDoZUKlUYsOGDWjSpAnat2+PsWPHaupYWFhIESIREVGhqFSqfL/0XzV8+HBNbwAAeHt748aNGwgLC0NwcDAcHR0BAHfv3oWTk5PmcXfv3kWNGjUAAI6Ojrh3755Wu8+fP8eDBw80j38TSYYDrKysYG1trXXz9PTErVu3sHjxYlhbW2vqEBER6YtUqwMyMjKgVGp/BRsYGECtVgMAKlSoAEdHR+zdu1ezPS0tDdHR0fDz8wMA+Pn5ISUlBSdPntTU2bdvH9RqNWrVqlWgOCTpCdi/f78UuyUiItIi1cmCWrdujalTp6J8+fKoWrUqTp8+jdmzZ6Nnz56auAYPHowpU6agcuXKqFChAsaOHQtnZ2e0bdsWAODh4YFmzZqhd+/eWLx4MZ49e4YBAwagc+fOcHZ2LlAckiQB/v7+UuyWiIjovbBgwQKMHTsWX3/9Ne7duwdnZ2f07dsX48aN09QZMWIEHj9+jD59+iAlJQWffvopdu/eDWNjY02dNWvWYMCAAWjcuDGUSiXat2+P+fPnFzgOhXj59EQSWLlyJczMzNChQwet8o0bNyIjI+OtJ0iYNJqqi/CI3ltP9n2Hp8/fXI+oODPW809Vy66rddZW6tr/6aytoiL5EsGwsDCUKVMmV7m9vT2mTZsmQURERCQXUs0JeF9IngTEx8ejQoUKucpdXFwQHx8vQURERETyIHkSYG9vj7Nnz+YqP3PmDGxtbSWIiIiI5ELuPQGSnyyoS5cuGDhwIMzNzVG/fn0AwMGDBzFo0CDN+kkiIiJ9KK5f3roieRIwefJkXL9+HY0bN0apUi/CUavVCAoK4pwAIiIiPZI8CTAyMsKGDRswefJknDlzBiYmJvD29oaLi4vUoRERUQnHnoD3xIcffogPP/xQ6jCIiEhO5J0DvB9JwK1bt7Bt2zbEx8cjKytLa9vs2bMlioqIiKhkkzwJ2Lt3Lz777DNUrFgRly5dgpeXF65fvw4hBD766COpwyMiohJM7sMBki8RHD16NIYNG4Zz587B2NgYmzZtws2bN+Hv75/rLIJERES6JPclgpInARcvXkRQUBAAoFSpUnjy5AnMzMwwadIkTJ8+XeLoiIiISi7JkwBTU1PNPAAnJydcvXpVsy05OVmqsIiISAbk3hMg+ZyA2rVr4/Dhw/Dw8ECLFi0wdOhQnDt3Dps3b0bt2rWlDo+IiEqy4vndrTOSJwGzZ89Geno6AGDixIlIT0/Hhg0bULlyZa4MICIi0iPJk4CKFStq/m9qaorFixdLGA0REclJce3G1xXJ5wRUrFgR9+/fz1WekpKilSAQERHpmtznBEieBFy/fh3Z2dm5yjMzM3H79m0JIiIiIpIHyYYDtm3bpvn/nj17YGlpqbmfnZ2NvXv3wtXVVYLIiIhILorrL3hdkSwJaNu2reb/wcHBWtsMDQ3h6uqKWbNmFXFUREQkJ0wCJKJWqwEAFSpUwIkTJ1CmTBmpQiEiIpIlyecETJw4Eebm5rnKs7Ky8Msvv0gQERERyYZCh7diSPIkICQkBKmpqbnKHz16hJCQEAkiIiIiueDqAIkJIfJ88m7duqU1WZCIiIh0S7I5AT4+PprsqXHjxihV6v9Cyc7ORlxcHJo1ayZVeEREJAPF9Re8rki+OiAmJgaBgYEwMzPTbDMyMoKrqyvat28vUXRERCQHTAIkMn78eACAq6srOnXqBGNj41x1zp8/Dy8vr6IOjYiISBYknxMQHByslQA8evQIS5cuxSeffILq1atLGBkREZV4XB3wfoiMjERwcDCcnJwwc+ZMNGrUCMeOHZM6LCIiKsHkvjpA0qsIJiYmYtWqVVi+fDnS0tLQsWNHZGZmYuvWrfD09JQyNCIiohJPsp6A1q1bw93dHWfPnsXcuXNx584dLFiwQKpwiIhIhtgTIJFdu3Zh4MCB6NevHypXrixVGJQPpVKBMcH10SXACw42pki4n47Vu8/i+18Pa+o82fddno/9dslezNnwYijH2twYs78JRAu/ylALga2RlzBs4V94/PRZkRwHka6sX7sG4SuXIzk5CR+6V8Gob8fCu1o1qcOid1Rcv7x1RbKegMOHD+PRo0fw9fVFrVq1sHDhQiQnJ0sVDr1iaGc/9P7sIwyZvwc1eizBmKX7ENq5Nr7+vKamjmv7uVq3PjP+hFotsCXykqbOym/bwsPVDq2Gr0X7bzfg02rl8ePQFlIcEtFb271rJ2bOCEPfr/tj/cYtcHevgn59e+H+/ftSh0b0TiRLAmrXro1ly5YhISEBffv2xfr16+Hs7Ay1Wo2IiAg8evRIqtAIQO2qZbH9yH/YHX0F8XdTsSXyEvb+E4eaVZw1de4+fKx1a13nQxyMuY7rCSkAAPfytgisVQlfz9yBE5fu4Oj5WwhdsAcdGlaFk61ZPnsmev+sDl+Jdl90RNvP26OSmxvGjJ8IY2NjbN28SerQ6B3JfThA8tUBpqam6NmzJw4fPoxz585h6NCh+P7772Fvb4/PPvtM6vBk69iFW2j4kSvcytoAALwr2sPPqyz+On41z/r21qZoVtsN4TvPaMpqeZbFw0dPcOq/BE3ZvpNxUAuBjz0+0O8BEOnIs6wsXPz3Amr71dGUKZVK1K5dB2fPnJYwMtIJmS8RlHR1wKvc3d0xY8YMhIWF4c8//8SKFSve+JjMzExkZmZqlalUKn2FKBsz1x2FhakKZ1Z9hWy1GgZKJcYvP4D1ey/kWb97U288ysjC1kP/NxTgYGOKpJQMrXrZaoEHaU/gYGOq1/iJdOVhykNkZ2fD1tZWq9zW1hZxcdckiopIN96rJCCHgYEB2rZtqzm18OuEhYVh4sSJWmUvzkZoqJ/gZOKLBp7o3NgLPaZuxb/Xk1DNzQE/fN0ECfcfYc1f53LVD2peHRv2nkfms2wJoiUiejvFtRtfV97LJKAwRo8ejdDQUK0ylUqF6ZEzJYqoZJjWtzFmrjuKjfv/BQBciEtCeQdLDO9aJ1cSUNe7HNzLl8H/Jm3RKr/74DHsrEprlRkoFbCxMMHdB4/1ewBEOmJtZQ0DA4NckwDv37+PMmXKSBQV6YrckwDJ5wS8K5VKBQsLC60bhwPenYmqFNRCaJVlZwso83jDBDevjpOxCTh37Z5WefS/t2BtbgKfyo6asgYfuUKpUODExdv6CZxIxwyNjODhWRXRx6I0ZWq1GtHRUahW3UfCyIjeXbHvCSD92Bl1GSO71cXNu2n493oSalR2xMAOn+CXXWe06pmXNkI7fw+MWrw3Vxux8fexJ/oqfhzWEgPn7IKhgRJzvgnExv0XkHA/vagOheid/S84BGO/HYmqVb3g5V0Nv64Ox5MnT9D283ZSh0bvSOYdAUwCKG+hC/7C+J7+mDe4GeysSiPhfjqWbz+Nab8c0qrXoWFVKBQK/LYv7wmDIdO2Ys7AQOyc2RVqtcDWQ7EYumBPURwCkc40a94CDx88wKKF85GcnAT3Kh5YtORn2HI4oNiT+3CAQohX+nyLwLZt2wpc922XCZo0mvpWjyMqLp7s+w5Pn0sdBZF+Gev5p2rl4bt11tblH5rprK2iIklPQEFm/QMvMrTsbM42JyIi/ZB5R4A0SYBarZZit0RERFrkPhxQ7FcHEBER0dt5LyYGPn78GAcPHkR8fDyysrK0tg0cOFCiqIiIqKSTeUeA9EnA6dOn0aJFC2RkZODx48ewsbFBcnIySpcuDXt7eyYBRESkN0qlvLMAyYcDhgwZgtatW+Phw4cwMTHBsWPHcOPGDfj6+mLmTJ71j4iISF8kTwJiYmIwdOhQKJVKGBgYIDMzE+XKlcOMGTPw7bffSh0eERGVYAqF7m7FkeRJgKGhIZTKF2HY29sjPj4eAGBpaYmbN29KGRoREVGJJvmcAB8fH5w4cQKVK1eGv78/xo0bh+TkZKxevRpeXl5Sh0dERCUYlwhKbNq0aXBycgIATJ06FdbW1ujXrx+SkpKwdOlSiaMjIqKSTO7DAZL3BNSsWVPzf3t7e+zerbtTOBIREVH+JE8CiIiIpCL34QDJk4AKFSq89o9w7dq1IoyGiIjkhEmAxAYPHqx1/9mzZzh9+jR2796N4cOHSxMUERGRDEieBAwaNCjP8h9//BH//PNPEUdDRERyIvOOAOlXB+SnefPm2LRpk9RhEBFRCaZQKHR2K47e2yTg999/h42NjdRhEBERlViSDwf4+PhoZVBCCCQmJiIpKQmLFi2SMDIiIirpiukPeJ2RPAlo06aNVhKgVCphZ2eHBg0aoEqVKhJGRkREJV1x7cbXFcmTgAkTJkgdAhERkSxJPifAwMAA9+7dy1V+//59GBgYSBARERHJBU8bLDEhRJ7lmZmZMDIyKuJoiIhITjgcIJH58+cDePEH+Pnnn2FmZqbZlp2djcjISM4JICIi0iPJkoA5c+YAeNETsHjxYq2ufyMjI7i6umLx4sVShUdERDIg844A6ZKAuLg4AEDDhg2xefNmWFtbSxUKERHJFIcDJLZ//36pQyAiIpIlyVcHtG/fHtOnT89VPmPGDHTo0EGCiIiISC7kvjpA8iQgMjISLVq0yFXevHlzREZGShARERHJBa8dILH09PQ8lwIaGhoiLS1NgoiIiIjkQfIkwNvbGxs2bMhVvn79enh6ekoQERERyQWHAyQ2duxYTJ48GcHBwQgPD0d4eDiCgoIwdepUjB07VurwiIioBJNyOOD27dvo3r07bG1tYWJiAm9vb/zzzz+a7UIIjBs3Dk5OTjAxMUFAQAAuX76s1caDBw/QrVs3WFhYwMrKCr169UJ6enqBY5A8CWjdujW2bt2KK1eu4Ouvv8bQoUNx69Yt/P3332jbtq3U4REREencw4cPUbduXRgaGmLXrl34999/MWvWLK3l8jNmzMD8+fOxePFiREdHw9TUFIGBgXj69KmmTrdu3XDhwgVERERg+/btiIyMRJ8+fQoch0Lkd97e98D58+fh5eX1Vo81aTRVx9EQvV+e7PsOT59LHQWRfhnreSH7pzMP6aytw8PqFbjuqFGjcOTIERw6lPf+hRBwdnbG0KFDMWzYMABAamoqHBwcsGrVKnTu3BkXL16Ep6cnTpw4gZo1awIAdu/ejRYtWuDWrVtwdnZ+YxyS9wS86tGjR1i6dCk++eQTVK9eXepwiIioBNPlcEBmZibS0tK0bpmZmXnud9u2bahZsyY6dOgAe3t7+Pj4YNmyZZrtcXFxSExMREBAgKbM0tIStWrVQlRUFAAgKioKVlZWmgQAAAICAqBUKhEdHV2g439vkoDIyEgEBQXByckJM2fORKNGjXDs2DGpwyIiIiqQsLAwWFpaat3CwsLyrHvt2jX89NNPqFy5Mvbs2YN+/fph4MCBCA8PBwAkJiYCABwcHLQe5+DgoNmWmJgIe3t7re2lSpWCjY2Nps6bSHrGwMTERKxatQrLly9HWloaOnbsiMzMTGzdupUrA4iISO90ub5/9OjRCA0N1SpTqVR51lWr1ahZsyamTZsGAPDx8cH58+exePFiBAcH6yymN5GsJ6B169Zwd3fH2bNnMXfuXNy5cwcLFiyQKhwiIpIhXS4RVKlUsLCw0LrllwQ4OTnl+rHr4eGB+Ph4AICjoyMA4O7du1p17t69q9nm6OiIe/fuaW1//vw5Hjx4oKnzJpIlAbt27UKvXr0wceJEtGzZUusqgkRERCVZ3bp1ERsbq1X233//wcXFBQBQoUIFODo6Yu/evZrtaWlpiI6Ohp+fHwDAz88PKSkpOHnypKbOvn37oFarUatWrQLFIVkScPjwYTx69Ai+vr6oVasWFi5ciOTkZKnCISIiGZLqPAFDhgzBsWPHMG3aNFy5cgVr167F0qVL0b9/f01cgwcPxpQpU7Bt2zacO3cOQUFBcHZ21iyf9/DwQLNmzdC7d28cP34cR44cwYABA9C5c+cCrQwAJEwCateujWXLliEhIQF9+/bF+vXr4ezsDLVajYiICDx69Eiq0IiISCakOmPgxx9/jC1btmDdunXw8vLC5MmTMXfuXHTr1k1TZ8SIEfjmm2/Qp08ffPzxx0hPT8fu3bthbGysqbNmzRpUqVIFjRs3RosWLfDpp59i6dKlBT/+9+k8AbGxsVi+fDlWr16NlJQUNGnSBNu2bXurtnieACrpeJ4AkgN9nyeg4byjOmtr/6A6OmurqLw3SwQBwN3dHTNmzMCtW7ewbt06qcMhIqISTu5XEZR0iWB+DAwM0LZtW542mIiI9KqYfnfrzHvVE0BERERF573sCSAiIioKSpl3BTAJICIi2ZJ5DsDhACIiIrliTwAREclWcZ3VrytMAoiISLaU8s4BOBxAREQkV+wJICIi2eJwABERkUzJPAfgcAAREZFcsSeAiIhkSwF5dwUwCSAiItni6gAiIiKSJfYEEBGRbHF1QAGcPXu2wA1Wq1btrYMhIiIqSjLPAQqWBNSoUQMKhQJCiDy352xTKBTIzs7WaYBERESkHwVKAuLi4vQdBxERUZHjpYQLwMXFRd9xEBERFTmZ5wBvtzpg9erVqFu3LpydnXHjxg0AwNy5c/HHH3/oNDgiIiLSn0InAT/99BNCQ0PRokULpKSkaOYAWFlZYe7cubqOj4iISG8UCoXObsVRoZOABQsWYNmyZfjuu+9gYGCgKa9ZsybOnTun0+CIiIj0SaHQ3a04KnQSEBcXBx8fn1zlKpUKjx8/1klQREREpH+FTgIqVKiAmJiYXOW7d++Gh4eHLmIiIiIqEkqFQme34qjQZwwMDQ1F//798fTpUwghcPz4caxbtw5hYWH4+eef9REjERGRXhTPr27dKXQS8OWXX8LExARjxoxBRkYGunbtCmdnZ8ybNw+dO3fWR4xERESkB2917YBu3bqhW7duyMjIQHp6Ouzt7XUdFxERkd4V11n9uvLWFxC6d+8eYmNjAbx4Eu3s7HQWFBERUVHgpYQL6dGjR/jf//4HZ2dn+Pv7w9/fH87OzujevTtSU1P1ESMRERHpQaGTgC+//BLR0dHYsWMHUlJSkJKSgu3bt+Off/5B37599REjERGRXsj9ZEGFHg7Yvn079uzZg08//VRTFhgYiGXLlqFZs2Y6DY6IiEifiul3t84UuifA1tYWlpaWucotLS1hbW2tk6CIiIhI/wqdBIwZMwahoaFITEzUlCUmJmL48OEYO3asToMjIiLSJw4HFICPj4/WAV6+fBnly5dH+fLlAQDx8fFQqVRISkrivAAiIio25L46oEBJQNu2bfUcBhERERW1AiUB48eP13ccRERERa64duPrylufLIiIiKi4k3cK8BZJQHZ2NubMmYPffvsN8fHxyMrK0tr+4MEDnQVHRERE+lPo1QETJ07E7Nmz0alTJ6SmpiI0NBTt2rWDUqnEhAkT9BAiERGRfsj9UsKFTgLWrFmDZcuWYejQoShVqhS6dOmCn3/+GePGjcOxY8f0ESMREZFeKBS6uxVHhU4CEhMT4e3tDQAwMzPTXC+gVatW2LFjh26jIyIiIr0pdBJQtmxZJCQkAAAqVaqEv/76CwBw4sQJqFQq3UZHRESkR3I/WVChk4DPP/8ce/fuBQB88803GDt2LCpXroygoCD07NlT5wESERHpi9yHAwq9OuD777/X/L9Tp05wcXHB0aNHUblyZbRu3VqnwREREZH+FLon4FW1a9dGaGgoatWqhWnTpukiJiIioiLB1QE6kpCQwAsIERFRsSL34QCdJQFERERUvPC0wUREJFvFdVa/rpTYJODJvu+kDoFI74xL7DuYqGjIvTu8wB8hoaGhr92elJT0zsHo0tPnUkdApF/GpQATnwFSh0GkV09OL5Q6hBKtwEnA6dOn31infv367xQMERFRUeJwQAHt379fn3EQEREVOaW8cwDZD4cQERHJFqcVERGRbMm9J4BJABERyZbc5wRwOICIiEim2BNARESyJffhgLfqCTh06BC6d+8OPz8/3L59GwCwevVqHD58WKfBERER6ROvHVBImzZtQmBgIExMTHD69GlkZmYCAFJTU3kVQSIiomKk0EnAlClTsHjxYixbtgyGhoaa8rp16+LUqVM6DY6IiEif5H4p4ULPCYiNjc3zzICWlpZISUnRRUxERERFQu6z4wt9/I6Ojrhy5Uqu8sOHD6NixYo6CYqIiIj0r9BJQO/evTFo0CBER0dDoVDgzp07WLNmDYYNG4Z+/frpI0YiIiK9kPvEwEIPB4waNQpqtRqNGzdGRkYG6tevD5VKhWHDhuGbb77RR4xERER6UVzH8nWl0EmAQqHAd999h+HDh+PKlStIT0+Hp6cnzMzM9BEfERER6clbnyzIyMgInp6euoyFiIioSMm8I6DwSUDDhg1fe67lffv2vVNARERERUXuZwwsdBJQo0YNrfvPnj1DTEwMzp8/j+DgYF3FRURERHpW6CRgzpw5eZZPmDAB6enp7xwQERFRUZH7xECdnSehe/fuWLFiha6aIyIi0ju5LxHUWRIQFRUFY2NjXTVHREREelboJKBdu3Zat88//xy1a9dGSEgI+vbtq48YiYiI9EKp0N3tbX3//fdQKBQYPHiwpuzp06fo378/bG1tYWZmhvbt2+Pu3btaj4uPj0fLli1RunRp2NvbY/jw4Xj+/Hmh9l3oOQGWlpZa95VKJdzd3TFp0iQ0bdq0sM0RERFJRgFp+/FPnDiBJUuWoFq1alrlQ4YMwY4dO7Bx40ZYWlpiwIABaNeuHY4cOQIAyM7ORsuWLeHo6IijR48iISEBQUFBMDQ0LNQVfQuVBGRnZyMkJATe3t6wtrYuzEOJiIjoJenp6ejWrRuWLVuGKVOmaMpTU1OxfPlyrF27Fo0aNQIArFy5Eh4eHjh27Bhq166Nv/76C//++y/+/vtvODg4oEaNGpg8eTJGjhyJCRMmwMjIqEAxFGo4wMDAAE2bNuXVAomIqETQ5XBAZmYm0tLStG6ZmZn57rt///5o2bIlAgICtMpPnjyJZ8+eaZVXqVIF5cuXR1RUFIAX8/C8vb3h4OCgqRMYGIi0tDRcuHCh4Mdf4Jr/n5eXF65du1bYhxEREb13dJkEhIWFwdLSUusWFhaW537Xr1+PU6dO5bk9MTERRkZGsLKy0ip3cHBAYmKips7LCUDO9pxtBVXoOQFTpkzBsGHDMHnyZPj6+sLU1FRru4WFRWGbJCIiKvZGjx6N0NBQrTKVSpWr3s2bNzFo0CBERERIvqquwEnApEmTMHToULRo0QIA8Nlnn2mdPlgIAYVCgezsbN1HSUREpAevOw1+YalUqjy/9F918uRJ3Lt3Dx999JGmLDs7G5GRkVi4cCH27NmDrKwspKSkaPUG3L17F46OjgAAR0dHHD9+XKvdnNUDOXUKosBJwMSJE/HVV19h//79BW6ciIjofSbFtQMaN26Mc+fOaZWFhISgSpUqGDlyJMqVKwdDQ0Ps3bsX7du3BwDExsYiPj4efn5+AAA/Pz9MnToV9+7dg729PQAgIiICFhYWhbq4X4GTACEEAMDf37/AjRMREZE2c3NzeHl5aZWZmprC1tZWU96rVy+EhobCxsYGFhYW+Oabb+Dn54fatWsDAJo2bQpPT0/873//w4wZM5CYmIgxY8agf//+BeqNyFGoOQG67DYhIiKS2vv6tTZnzhwolUq0b98emZmZCAwMxKJFizTbDQwMsH37dvTr1w9+fn4wNTVFcHAwJk2aVKj9KETOT/w3UCqVsLS0fGMi8ODBg0IFoC9PC3fSJKJix7gUYOIzQOowiPTqyemFem1/7qE4nbU1uF4FnbVVVArVEzBx4sRcZwwkIiKi4qlQSUDnzp01ExCIiIiKOykmBr5PCpwEcD4AERGVNHL/aivwGQMLOHWAiIiIiokC9wSo1Wp9xkFERFTklBJfRVBqhT5tMBERUUnB4QAiIiKSJfYEEBGRbHF1ABERkUwpZT4ewOEAIiIimWJPABERyZbMOwKYBBARkXxxOICIiIhkiT0BREQkWzLvCGASQERE8iX37nBJkoC0tLQC17WwsNBjJERERPIlSRJgZWX1xqsSCiGgUCiQnZ1dRFEREZHcyP0KuZIkAfv375dit0RERFrknQJIlAT4+/tLsVsiIiJ6yXszMTAjIwPx8fHIysrSKq9WrZpEERERUUkn9/MESJ4EJCUlISQkBLt27cpzO+cEEBGRvsg7BXgPVkcMHjwYKSkpiI6OhomJCXbv3o3w8HBUrlwZ27Ztkzo8IiKiEkvynoB9+/bhjz/+QM2aNaFUKuHi4oImTZrAwsICYWFhaNmypdQhEhFRCSXz0QDpewIeP34Me3t7AIC1tTWSkpIAAN7e3jh16pSUoRERUQmnUCh0diuOJE8C3N3dERsbCwCoXr06lixZgtu3b2Px4sVwcnKSODoiIqKSS/LhgEGDBiEhIQEAMH78eDRr1gxr1qyBkZERVq1aJW1wRERUokn+S1hikicB3bt31/zf19cXN27cwKVLl1C+fHmUKVNGwsiIiKikK67d+LoieRLwqtKlS+Ojjz6SOgwiIqIST/IkQAiB33//Hfv378e9e/egVqu1tm/evFmiyIiIqKSTdz/Ae5AEDB48GEuWLEHDhg3h4OAg+64ZIiIqOnL/zpE8CVi9ejU2b96MFi1aSB0KERGRrEieBFhaWqJixYpSh0FERDIk99UBkh//hAkTMHHiRDx58kTqUIiISGbkfrIgyXsCOnbsiHXr1sHe3h6urq4wNDTU2s6zBhIREemH5ElAcHAwTp48ie7du3NiIBERFSm5f+NIngTs2LEDe/bswaeffip1KEREJDNy/90p+ZyAcuXKwcLCQuowiIiIZEfyJGDWrFkYMWIErl+/LnUoREQkM0oodHYrjiQfDujevTsyMjJQqVIllC5dOtfEwAcPHkgUGRERlXRyHw6QPAmYO3eu1CEQERHJkqRJwLNnz3Dw4EGMHTsWFSpUkDIUIiKSIUUx7cbXFUnnBBgaGmLTpk1ShkBERDKmUOjuVhxJPjGwbdu22Lp1q9RhEBERyY7kcwIqV66MSZMm4ciRI/D19YWpqanW9oEDB0oUGRERlXTFdVa/riiEEELKAF43F0ChUODatWtv1e7T528bEVHxYFwKMPEZIHUYRHr15PRCvba/598knbUV6Gmns7aKiuQ9AXFxcVKHQEREJEuSJwEvy+mU4PUDiIioKMj960byiYEA8Msvv8Db2xsmJiYwMTFBtWrVsHr1aqnDIiKiEk6hw3/FkeQ9AbNnz8bYsWMxYMAA1K1bFwBw+PBhfPXVV0hOTsaQIUMkjpCIiKhkkjwJWLBgAX766ScEBQVpyj777DNUrVoVEyZMYBJARER6oyyeP+B1RvIkICEhAXXq1MlVXqdOHSQkJEgQERERyUVx7cbXFcnnBLi5ueG3337LVb5hwwZUrlxZgoiIiIjkQfKegIkTJ6JTp06IjIzUzAk4cuQI9u7dm2dyQEREpCtyXx0geRLQvn17REdHY86cOZrTB3t4eOD48ePw8fGRNjgiIirR5D4cIHkSAAC+vr749ddfpQ6DiIhIVt6LJICIiEgKXB0gEaVS+cYzAyoUCjx/zosAEBGRfnA4QCJbtmzJd1tUVBTmz58PtVpdhBHR21q/dg3CVy5HcnISPnSvglHfjoV3tWpSh0VUIGalVRj/dSt81qg67KzNcCb2FobN+B0n/40HAHzXtwU6BH6Eso7WyHqWjdMX4zFh4Z84cf6Gpg1ri9KYPbIDWtT3gloIbN0bg2EzfsfjJ1lSHRZRgUh+FcGXxcbGYtSoUfjzzz/RrVs3TJo0CS4uLm/VFq8iWDR279qJMaNHYMz4ifD2ro41q8Px11+78cf23bC1tZU6vBKNVxHUjdXfh8DTzRkDp61HQlIqurT4BN90a4iP2k/BnaRUdGpWE/cePkLcrWSYqAzxTfdGaBfgA682E5H8MB0AsHVhPziWscQ3U9bBsJQBlkzsjpMX4tHj21XSHlwJoO+rCB6+/FBnbX1a2VpnbRUVyc8TAAB37txB79694e3tjefPnyMmJgbh4eFvnQBQ0VkdvhLtvuiItp+3RyU3N4wZPxHGxsbYunmT1KERvZGxyhBtG9fAd3O34sipq7h2MxlTl+zE1ZtJ6N2hHgBgw+5/sD86Ftdv38fFa4kYOWszLM1N4FXZGQDgXsEBgXWr4utJa3Hi/A0cjbmG0Okb0SHwIzjZWUp5eFQACh3eiiNJk4DU1FSMHDkSbm5uuHDhAvbu3Ys///wTXl5eUoZFBfQsKwsX/72A2n7/d8ZHpVKJ2rXr4OyZ0xJGRlQwpQyUKFXKAE+znmmVP818hjo+lXLVNyxlgF7t6iLlUQbO/XcbAFCrWgU8TMvAqf8/fAAA+6JjoVYLfOzFHzL0fpNsTsCMGTMwffp0ODo6Yt26dWjTps1btZOZmYnMzEytMpVKBRiodBEmvcbDlIfIzs7O1e1va2uLuLhrEkVFVHDpGZk4duYaRvdujti4u7h7Pw0dm9VErWoVcPVmkqZe83pe+OX7EJQ2NkRichpafbUQ91MeAwAcbC2Q9OCRVrvZ2Wo8SMuAQxmLIj0eKjylzM8WJFkSMGrUKJiYmMDNzQ3h4eEIDw/Ps97mzZtf205YWBgmTpyoVTZ+/HiMGjNBV6ESUQnWc8wvWDKhG679NRXPn2cj5tJN/Lb7H/h4lNfUOXjiP9TqHIYyVmYIaVcHv87oifr/m4mk/z8ngIoveacAEiYBQUFBb1wiWBCjR49GaGioVplKpcJ7M9uxBLO2soaBgQHu37+vVX7//n2UKVNGoqiICifuVjKafjkPpY2NYGFmjMTkNKz+PgRxt5M1dTKeZuHazWRcu5mM4+eu49wf4xD8eR3MXPEX7t5Pg52NuVabBgZK2FiUxt3ktKI+HKJCkSwJWLVqlU7aUalUL7r/X8HVAfpnaGQED8+qiD4WhUaNAwAAarUa0dFR6Nylu8TRERVOxtMsZDzNgpW5CQLqeOC7uX/kW1epUEBl+OLjM/psHKwtSsPHoxxOX7wJAGjw8YdQKhVaywjpPSXzrgCeMZDeyf+CQzD225GoWtULXt7V8OvqcDx58gRtP28ndWhEBRLg5wGFAvjv+j1UKmeHaUPa4r+4u/hlWxRKGxth5JeB2HHwHBKTU2FrZYa+HevD2d4KmyNOAQBi4+5iz5EL+HFsVwycuh6GpQwwZ1RHbNxzCglJqRIfHb0JTxZE9A6aNW+Bhw8eYNHC+UhOToJ7FQ8sWvIzbDkcQMWEpZkxJn3zGT5wsMKD1Az8sTcG43/8E8+fq2GgVMPd1QHdW9eCrZUpHqRm4J8LNxDQcw4uXkvUtBHybTjmjOqInUu+gVr94mRBQ2dslPCoiArmvTpZkC5xOIBKOp4siORA3ycLOn5Nd701n1QsfueFYE8AERHJlrwHA96TMwYSERFR0ZOkJ2Dbtm0FrvvZZ5/pMRIiIpI1mXcFSJIEtG3btkD1FAoFsrOz9RsMERHJltxXB0gyHKBWqwt0YwJAREQlUVhYGD7++GOYm5vD3t4ebdu2RWxsrFadp0+fon///rC1tYWZmRnat2+Pu3fvatWJj49Hy5YtUbp0adjb22P48OF4/rzgM+M5J4CIiGRLodDdrTAOHjyI/v3749ixY4iIiMCzZ8/QtGlTPH78WFNnyJAh+PPPP7Fx40YcPHgQd+7cQbt2/3cOluzsbLRs2RJZWVk4evQowsPDsWrVKowbN67gx/8+LBF8/PgxDh48iPj4eGRlZWltGzhw4Fu1ySWCVNJxiSDJgb6XCJ68rrtTO/u6vv0Fo5KSkmBvb4+DBw+ifv36SE1NhZ2dHdauXYsvvvgCAHDp0iV4eHggKioKtWvXxq5du9CqVSvcuXMHDg4OAIDFixdj5MiRSEpKgpGR0Rv3K/kSwdOnT6NFixbIyMjA48ePYWNjg+TkZE3XxtsmAURERG+iyxkB+V3VNq9T278qNfXF+QpsbGwAACdPnsSzZ88QEBCgqVOlShWUL19ekwRERUXB29tbkwAAQGBgIPr164cLFy7Ax8fnjfuVfDhgyJAhaN26NR4+fAgTExMcO3YMN27cgK+vL2bOnCl1eEREVJIpdHcLCwuDpaWl1i0sLOyNIajVagwePBh169aFl5cXACAxMRFGRkawsrLSquvg4IDExERNnZcTgJztOdsKQvKegJiYGCxZsgRKpRIGBgbIzMxExYoVMWPGDAQHB2uNfxAREb2v8ruq7Zv0798f58+fx+HDh/UVWr4k7wkwNDSEUvkiDHt7e8THxwMALC0tcfPmTSlDIyKiEk6hw38qlQoWFhZatzclAQMGDMD27duxf/9+lC1bVlPu6OiIrKwspKSkaNW/e/cuHB0dNXVeXS2Qcz+nzptIngT4+PjgxIkTAAB/f3+MGzcOa9asweDBgzXdIkRERPog1eoAIQQGDBiALVu2YN++fahQoYLWdl9fXxgaGmLv3r2astjYWMTHx8PPzw8A4Ofnh3PnzuHevXuaOhEREbCwsICnp2eB4pA8CZg2bRqcnJwAAFOnToW1tTX69euHpKQkLF26VOLoiIiIdK9///749ddfsXbtWpibmyMxMRGJiYl48uQJgBe94b169UJoaCj279+PkydPIiQkBH5+fqhduzYAoGnTpvD09MT//vc/nDlzBnv27MGYMWPQv3//Ag1DAO/JEkF94BJBKum4RJDkQN9LBM/EP9JZW9XLmxe4riKfroOVK1eiR48eAF6cLGjo0KFYt24dMjMzERgYiEWLFml19d+4cQP9+vXDgQMHYGpqiuDgYHz//fcoVapgU/6YBBAVU0wCSA70ngTc1GESUK7gScD7QvLVARUqVMg3IwKAa9euFWE0RERE8iF5EjB48GCt+8+ePcPp06exe/duDB8+XJqgiIhIFuR+ASHJk4BBgwblWf7jjz/in3/+KeJoiIhITgo7q7+kkXx1QH6aN2+OTZs2SR0GERFRiSV5T0B+fv/9d805lImIiPRB5h0B0icBPj4+WhMDhRBITExEUlISFi1aJGFkRERU4sk8C5A8CWjTpo1WEqBUKmFnZ4cGDRqgSpUqEkZGRERUskmeBEyYMEHqEIiISKbkvjpA8omBBgYGWuc9znH//n0YGBhIEBEREcmFVNcOeF9IngTkd8LCzMxMGBkZFXE0RERE8iHZcMD8+fMBvDh/8s8//wwzMzPNtuzsbERGRnJOABER6VUx/QGvM5IlAXPmzAHwoidg8eLFWl3/RkZGcHV1xeLFi6UKj4iI5EDmWYBkSUBcXBwAoGHDhti8eTOsra2lCoWIiEiWJF8dsH//fqlDICIimeLqAIm1b98e06dPz1U+Y8YMdOjQQYKIiIhILrg6QGKRkZFo0aJFrvLmzZsjMjJSgoiIiIjkQfLhgPT09DyXAhoaGiItLU2CiIiISC6K6Q94nZG8J8Db2xsbNmzIVb5+/Xp4enpKEBEREcmGQoe3YkjynoCxY8eiXbt2uHr1Kho1agQA2Lt3L9atW4eNGzdKHB0REVHJJXkS0Lp1a2zduhXTpk3D77//DhMTE1SrVg1///03/P39pQ6PiIhKMLmvDpA8CQCAli1bomXLlrnKz58/Dy8vLwkiIiIiOSius/p1RfI5Aa969OgRli5dik8++QTVq1eXOhwiIqIS671JAiIjIxEUFAQnJyfMnDkTjRo1wrFjx6QOi4iISjCZzwuUdjggMTERq1atwvLly5GWloaOHTsiMzMTW7du5coAIiLSv+L67a0jkvUEtG7dGu7u7jh79izmzp2LO3fuYMGCBVKFQ0REJDuS9QTs2rULAwcORL9+/VC5cmWpwiAiIhmT++oAyXoCDh8+jEePHsHX1xe1atXCwoULkZycLFU4REQkQ7x2gERq166NZcuWISEhAX379sX69evh7OwMtVqNiIgIPHr0SKrQiIiIZEEhhBBSB5EjNjYWy5cvx+rVq5GSkoImTZpg27Ztb9XW0+c6Do7oPWNcCjDxGSB1GER69eT0Qr22fz35qc7aci1jrLO2isp7s0QQANzd3TFjxgzcunUL69atkzocIiIq6WS+RvC96gnQJfYEUEnHngCSA733BNzXYU+AbfHrCXgvThtMREQkBbmvDmASQEREslVcZ/Xryns1J4CIiIiKDnsCiIhItmTeEcAkgIiI5IvDAURERCRL7AkgIiIZk3dXAJMAIiKSLQ4HEBERkSyxJ4CIiGRL5h0BTAKIiEi+OBxAREREssSeACIiki1eO4CIiEiu5J0DcDiAiIhIrtgTQEREsiXzjgAmAUREJF9cHUBERESyxJ4AIiKSLa4OICIikit55wAcDiAiIpIr9gQQEZFsybwjgEkAERHJF1cHEBERkSyxJ4CIiGSLqwOIiIhkisMBREREJEtMAoiIiGSKwwFERCRbHA4gIiIiWWJPABERyRZXBxAREckUhwOIiIhIltgTQEREsiXzjgAmAUREJGMyzwI4HEBERCRT7AkgIiLZ4uoAIiIimeLqACIiIpIl9gQQEZFsybwjgEkAERHJmMyzAA4HEBERSeDHH3+Eq6srjI2NUatWLRw/frzIY2ASQEREsqXQ4b/C2LBhA0JDQzF+/HicOnUK1atXR2BgIO7du6enI80bkwAiIpIthUJ3t8KYPXs2evfujZCQEHh6emLx4sUoXbo0VqxYoZ8DzQeTACIiIh3IzMxEWlqa1i0zMzNXvaysLJw8eRIBAQGaMqVSiYCAAERFRRVlyCV3YqBxiT2y909mZibCwsIwevRoqFQqqcORlSenF0odgmzwdV4y6fK7YsKUMEycOFGrbPz48ZgwYYJWWXJyMrKzs+Hg4KBV7uDggEuXLukuoAJQCCFEke6RSpy0tDRYWloiNTUVFhYWUodDpBd8ndObZGZm5vrlr1KpciWNd+7cwQcffICjR4/Cz89PUz5ixAgcPHgQ0dHRRRIvUIJ7AoiIiIpSXl/4eSlTpgwMDAxw9+5drfK7d+/C0dFRX+HliXMCiIiIipCRkRF8fX2xd+9eTZlarcbevXu1egaKAnsCiIiIilhoaCiCg4NRs2ZNfPLJJ5g7dy4eP36MkJCQIo2DSQC9M5VKhfHjx3OyFJVofJ2TLnXq1AlJSUkYN24cEhMTUaNGDezevTvXZEF948RAIiIimeKcACIiIpliEkBERCRTTAKIiIhkiklACdSjRw+0bdtWc79BgwYYPHhwkcdx4MABKBQKpKSkFPm+39aECRNQo0YNqcOgfPC1rX98D8gLk4Ai0qNHDygUCigUChgZGcHNzQ2TJk3C8+fP9b7vzZs3Y/LkyQWqK9WHW1hYGAwMDPDDDz/k2pbXB31J/hAubvjazpurqysUCgWOHTumVT548GA0aNCgSGIgehMmAUWoWbNmSEhIwOXLlzF06FBMmDAhzy894MUFJnTFxsYG5ubmOmtPH1asWIERI0YU+RW0SDf42s6bsbExRo4cqfN2nz17pvM2SZ6YBBQhlUoFR0dHuLi4oF+/fggICMC2bdsA/F8359SpU+Hs7Ax3d3cAwM2bN9GxY0dYWVnBxsYGbdq0wfXr1zVtZmdnIzQ0FFZWVrC1tcWIESPw6qrPV39JZ2ZmYuTIkShXrhxUKhXc3NywfPlyXL9+HQ0bNgQAWFtbQ6FQoEePHgBenM0qLCwMFSpUgImJCapXr47ff/9daz87d+7Ehx9+CBMTEzRs2FArztc5ePAgnjx5gkmTJiEtLQ1Hjx7VbOvRowcOHjyIefPmaX5tvi7O3bt349NPP9U8H61atcLVq1e19nfr1i106dIFNjY2MDU1Rc2aNfM9V/fVq1dRsWJFDBgwINfzSv+Hr+289enTB8eOHcPOnTvzraNWqzFp0iSULVsWKpVKs148x/Xr16FQKLBhwwb4+/vD2NgYa9as0Tyv06ZNg4ODA6ysrDQ9MMOHD4eNjQ3Kli2LlStXau1v5MiR+PDDD1G6dGlUrFgRY8eOZVIhY0wCJGRiYqL1q2jv3r2IjY1FREQEtm/fjmfPniEwMBDm5uY4dOgQjhw5AjMzMzRr1kzzuFmzZmHVqlVYsWIFDh8+jAcPHmDLli2v3W9QUBDWrVuH+fPn4+LFi1iyZAnMzMxQrlw5bNq0CQAQGxuLhIQEzJs3D8CL7vpffvkFixcvxoULFzBkyBB0794dBw8eBPDiA71du3Zo3bo1YmJi8OWXX2LUqFEFeh6WL1+OLl26wNDQEF26dMHy5cs12+bNmwc/Pz/07t0bCQkJSEhIeG2cjx8/RmhoKP755x/s3bsXSqUSn3/+OdRqNQAgPT0d/v7+uH37NrZt24YzZ85gxIgRmu0vO3v2LD799FN07doVCxcuhKKwFwyXMb62X6hQoQK++uorjB49Os/XGPDiNT5r1izMnDkTZ8+eRWBgID777DNcvnxZq96oUaMwaNAgXLx4EYGBgQCAffv24c6dO4iMjMTs2bMxfvx4tGrVCtbW1oiOjsZXX32Fvn374tatW5p2zM3NsWrVKvz777+YN28eli1bhjlz5hToeKgEElQkgoODRZs2bYQQQqjVahERESFUKpUYNmyYZruDg4PIzMzUPGb16tXC3d1dqNVqTVlmZqYwMTERe/bsEUII4eTkJGbMmKHZ/uzZM1G2bFnNvoQQwt/fXwwaNEgIIURsbKwAICIiIvKMc//+/QKAePjwoabs6dOnonTp0uLo0aNadXv16iW6dOkihBBi9OjRwtPTU2v7yJEjc7X1qtTUVGFiYiJiYmKEEEKcPn1amJmZiUePHuUZ/+vizEtSUpIAIM6dOyeEEGLJkiXC3Nxc3L9/P8/648ePF9WrVxdHjhwR1tbWYubMma9tn/jazo+Li4uYM2eOuHfvnjA3Nxe//PKLEEKIQYMGCX9/f009Z2dnMXXqVK3Hfvzxx+Lrr78WQggRFxcnAIi5c+dq1QkODhYuLi4iOztbU+bu7i7q1aunuf/8+XNhamoq1q1bl2+cP/zwg/D19dXcz3kPkDzwtMFFaPv27TAzM8OzZ8+gVqvRtWtXretMe3t7w8jISHP/zJkzuHLlSq4xz6dPn+Lq1atITU1FQkICatWqpdlWqlQp1KxZM9+u65iYGBgYGMDf37/AcV+5cgUZGRlo0qSJVnlWVhZ8fHwAABcvXtSKA0CBLoSxbt06VKpUCdWrVwcA1KhRAy4uLtiwYQN69epV4BhzXL58GePGjUN0dDSSk5M1v77i4+Ph5eWFmJgY+Pj4wMbGJt824uPj0aRJE0ydOlWSmefFEV/b+bOzs8OwYcMwbtw4dOrUSWtbWloa7ty5g7p162qV161bF2fOnNEqq1mzZq62q1atCqXy/zp0HRwc4OXlpblvYGAAW1tb3Lt3T1O2YcMGzJ8/H1evXkV6ejqeP3/OSyPLGJOAItSwYUP89NNPMDIygrOzM0qV0n76TU1Nte6np6fD19cXa9asydWWnZ3dW8VgYmJS6Mekp6cDAHbs2IEPPvhAa9u7nkd9+fLluHDhgtZzoVarsWLFirdKAlq3bg0XFxcsW7YMzs7OUKvV8PLy0nQxF+T47ezs4OzsjHXr1qFnz578gCwAvrZfLzQ0FIsWLcKiRYveuo1Xn0MAMDQ01LqvUCjyLMtJhqOiotCtWzdMnDgRgYGBsLS0xPr16zFr1qy3jouKN84JKEKmpqZwc3ND+fLlc31I5uWjjz7C5cuXYW9vDzc3N62bpaUlLC0t4eTkpDWp7fnz5zh58mS+bXp7e0OtVmvGO1+V82stOztbU+bp6QmVSoX4+PhccZQrVw4A4OHhgePHj2u19erSqFedO3cO//zzDw4cOICYmBjN7cCBA4iKisKlS5c0Mb0cT35x3r9/H7GxsRgzZgwaN24MDw8PPHz4UOtx1apVQ0xMDB48eJBvXCYmJti+fTuMjY0RGBiIR48evfY4iK/tNzEzM8PYsWMxdepUrdeThYUFnJ2dceTIEa36R44cgaenZ6H2URBHjx6Fi4sLvvvuO9SsWROVK1fGjRs3dL4fKj6YBLzHunXrhjJlyqBNmzY4dOgQ4uLicODAAQwcOFAz0WfQoEH4/vvvsXXrVly6dAlff/31a9dBu7q6Ijg4GD179sTWrVs1bf72228AABcXFygUCmzfvh1JSUlIT0+Hubk5hg0bhiFDhiA8PBxXr17FqVOnsGDBAoSHhwMAvvrqK1y+fBnDhw9HbGws1q5di1WrVr32+JYvX45PPvkE9evXh5eXl+ZWv359fPzxx5oJgq6uroiOjsb169c1Xfx5xWltbQ1bW1ssXboUV65cwb59+xAaGqq1zy5dusDR0RFt27bFkSNHcO3aNWzatAlRUVFa9UxNTbFjxw6UKlUKzZs31/xiJN0o6a/tvPTp0weWlpZYu3atVvnw4cMxffp0bNiwAbGxsRg1ahRiYmIwaNCgQu/jTSpXroz4+HisX78eV69exfz589842ZJKOKknJcjFy5OnCrM9ISFBBAUFiTJlygiVSiUqVqwoevfuLVJTU4UQLyZLDRo0SFhYWAgrKysRGhoqgoKC8p08JYQQT548EUOGDBFOTk7CyMhIuLm5iRUrVmi2T5o0STg6OgqFQiGCg4OFEC8mfM2dO1e4u7sLQ0NDYWdnJwIDA8XBgwc1j/vzzz+Fm5ubUKlUol69emLFihX5Tp7KzMwUtra2WhO/XjZ9+nRhb28vsrKyRGxsrKhdu7YwMTERAERcXFy+cUZERAgPDw+hUqlEtWrVxIEDBwQAsWXLFk3b169fF+3btxcWFhaidOnSombNmiI6OloIkXtS1KNHj0SdOnVE/fr1RXp6ep6xyh1f23nLmRj4srVr1woAWhMDs7OzxYQJE8QHH3wgDA0NRfXq1cWuXbs023MmBp4+ffqNz2tek2hfjWP48OHC1tZWmJmZiU6dOok5c+YIS0tLzXZODJQXXkqYiIhIpjgcQEREJFNMAoiIiGSKSQAREZFMMQkgIiKSKSYBREREMsUkgIiISKaYBBAREckUkwAiIiKZYhJApAc9evRA27ZtNfcbNGggyRUJDxw4AIVC8drT7b6rV4/1bRRFnESUG5MAko0ePXpAoVBAoVDAyMgIbm5umDRpEp4/f673fW/evBmTJ08uUN2i/kJ0dXXF3Llzi2RfRPR+4aWESVaaNWuGlStXIjMzEzt37kT//v1haGiI0aNH56qblZWlufLcu7KxsdFJO0REusSeAJIVlUoFR0dHuLi4oF+/fggICMC2bdsA/F+39tSpU+Hs7Ax3d3cAwM2bN9GxY0dYWVnBxsYGbdq0wfXr1zVtZmdnIzQ0FFZWVrC1tcWIESPw6iU5Xh0OyMzMxMiRI1GuXDmoVCq4ublh+fLluH79Oho2bAgAsLa2hkKhQI8ePQAAarUaYWFhqFChAkxMTFC9enX8/vvvWvvZuXMnPvzwQ5iYmKBhw4Zacb6N7Oxs9OrVS7NPd3d3zJs3L8+6EydOhJ2dHSwsLPDVV18hKytLs60gsRNR0WNPAMmaiYkJ7t+/r7m/d+9eWFhYICIiAgDw7NkzBAYGws/PD4cOHUKpUqUwZcoUNGvWDGfPnoWRkRFmzZqFVatWYcWKFfDw8MCsWbOwZcsWNGrUKN/9BgUFISoqCvPnz0f16tURFxeH5ORklCtXDps2bUL79u0RGxsLCwsLmJiYAADCwsLw66+/YvHixahcuTIiIyPRvXt32NnZwd/fHzdv3kS7du3Qv39/9OnTB//88w+GDh36Ts+PWq1G2bJlsXHjRtja2uLo0aPo06cPnJyc0LFjR63nzdjYGAcOHMD169cREhICW1tbTJ06tUCxE5FEJL6KIVGRefnSq2q1WkRERAiVSiWGDRum2e7g4CAyMzM1j1m9erVwd3cXarVaU5aZmSlMTEzEnj17hBBCODk5aV0S+dmzZ6Js2bL5XvI2NjZWABARERF5xrl///5cl6l9+vSpKF26tDh69KhW3V69eokuXboIIYQYPXq08PT01No+cuTIt7rk7ev0799ftG/fXnM/ODhY2NjYiMePH2vKfvrpJ2FmZiays7MLFHtex0xE+seeAJKV7du3w8zMDM+ePYNarUbXrl0xYcIEzXZvb2+teQBnzpzBlStXYG5urtXO06dPcfXqVaSmpiIhIQG1atXSbCtVqhRq1qyZa0ggR0xMDAwMDAr1C/jKlSvIyMhAkyZNtMqzsrLg4+MDALh48aJWHADg5+dX4H3k58cff8SKFSsQHx+PJ0+eICsrCzVq1NCqU716dZQuXVprv+np6bh58ybS09PfGDsRSYNJAMlKw4YN8dNPP8HIyAjOzs4oVUr7LWBqaqp1Pz09Hb6+vlizZk2utuzs7N4qhpzu/cJIT08HAOzYsQMffPCB1jaVSvVWcRTE+vXrMWzYMMyaNQt+fn4wNzfHDz/8gOjo6AK3IVXsRPRmTAJIVkxNTeHm5lbg+h999BE2bNgAe3t7WFhY5FnHyckJ0dHRqF+/PgDg+fPnOHnyJD766KM863t7e0OtVuPgwYMICAjItT2nJyI7O1tT5unpCZVKhfj4+Hx7EDw8PDSTHHMcO3bszQf5GkeOHEGdOnXw9ddfa8quXr2aq96ZM2fw5MkTTYJz7NgxmJmZoVy5crCxsXlj7EQkDa4OIHqNbt26oUyZMmjTpg0OHTqEuLg4HDhwAAMHDsStW7cAAIMGDcL333+PrVu34tKlS/j6669fu8bf1dUVwcHB6NmzJ7Zu3app87fffgMAuLi4QKFQYPv27UhKSkJ6ejrMzc0xbNgwDBkyBOHh4bh69SpOnTqFBQsWIDw8HADw1Vdf4fLlyxg+fDhiY2Oxdu1arFq1qkDHefv2bcTExGjdHj58iMqVK+Off/7Bnj178N9//2Hs2LE4ceJErsdnZWWhV69e+Pfff7Fz506MHz8eAwYMgFKpLFDsRCQRqSclEBWVlycGFmZ7QkKCCAoKEmXKlBEqlUpUrFhR9O7dW6SmpgohXkwEHDRokLCwsBBWVlYiNDRUBAUF5TsxUAghnjx5IoYMGSKcnJyEkZGRcHNzEytWrNBsnzRpknB0dBQKhUIEBwcLIV5MZpw7d65wd3cXhoaGws7OTgQGBoqDBw9qHvfnn38KNzc3oVKpRL169cSKFSsKNDEQQK7b6tWrxdOnT0WPHj2EpaWlsLKyEv369ROjRo0S1atXz/W8jRs3Ttja2gozMzPRu3dv8fTpU02dN8XOiYFE0lAIkc/sJSIiIirROBxAREQkU0wCiIiIZIpJABERkUwxCSAiIpIpJgFEREQyxSSAiIhIppgEEBERyRSTACIiIpliEkBERCRTTAKIiIhkikkAERGRTP0/sCqjRl8mrGQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual_labels_binary, predictions_binary)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "cm_df = pd.DataFrame(cm, index=[\"Actual Attack\", \"Actual Normal\"], columns=[\"Predicted Attack\", \"Predicted Normal\"])\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', linewidths=0.5)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - Llama Binary fine-tuned\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "threatlogllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
